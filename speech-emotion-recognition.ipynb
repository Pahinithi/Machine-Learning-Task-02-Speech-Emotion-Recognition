{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555},{"sourceId":6060815,"sourceType":"datasetVersion","datasetId":3468263}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Emotion Recognition from Speech Machine learning & Deep Learning \n","metadata":{}},{"cell_type":"markdown","source":"Build a model that can recognize emotions in speech audio. Use deep learning and speech processing techniques to classify spoken sentences into different emotions like **happiness, anger, or sadness**","metadata":{}},{"cell_type":"markdown","source":"# Import necessary libraries","metadata":{}},{"cell_type":"code","source":"\n#IMPORT THE LIBRARIES\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nimport IPython.display as ipd\nfrom IPython.display import Audio\nimport keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM,BatchNormalization , GRU\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import SGD\n\n\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nimport tensorflow as tf \nprint (\"Done\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-04T14:08:48.409061Z","iopub.execute_input":"2024-02-04T14:08:48.409462Z","iopub.status.idle":"2024-02-04T14:09:03.826211Z","shell.execute_reply.started":"2024-02-04T14:08:48.409434Z","shell.execute_reply":"2024-02-04T14:09:03.825242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Update package information and install libsndfile1 for audio processing.","metadata":{}},{"cell_type":"code","source":"!apt-get update\n!apt-get install -y libsndfile1","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:03.828173Z","iopub.execute_input":"2024-02-04T14:09:03.828738Z","iopub.status.idle":"2024-02-04T14:09:10.877310Z","shell.execute_reply.started":"2024-02-04T14:09:03.828712Z","shell.execute_reply":"2024-02-04T14:09:10.875857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data ","metadata":{}},{"cell_type":"markdown","source":"This is a guide to understand the filenames of audio files in the RAVDESS dataset. Each filename has information about:\n\n1. Type of recording (full-AV, video-only, audio-only).\n2. Vocal content (speech or song).\n3. Emotion expressed (neutral, calm, happy, sad, angry, fearful, disgust, surprised).\n4. Emotional intensity (normal or strong, except for 'neutral' which is always normal).\n5. Statement made (\"Kids are talking by the door\" or \"Dogs are sitting by the door\").\n6. Repetition of the statement (1st or 2nd).\n7. Actor involved (from 1 to 24, with odd numbers representing males and even numbers representing females).\n\nFor example, the filename \"02-01-06-01-02-01-12.mp4\" means:\n\n- It's a video-only recording.\n- Involves speech.\n- Expresses fear.\n- Has normal emotional intensity.\n- The statement is about dogs.\n- It's the first repetition of the statement.\n- Involves the 12th actor, who is female (as the actor ID is even).","metadata":{}},{"cell_type":"markdown","source":"# Load the  dataset","metadata":{}},{"cell_type":"code","source":"\n\nravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nravdess_directory_list = os.listdir(ravdess)\nprint(ravdess_directory_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:10.878974Z","iopub.execute_input":"2024-02-04T14:09:10.879376Z","iopub.status.idle":"2024-02-04T14:09:10.889859Z","shell.execute_reply.started":"2024-02-04T14:09:10.879334Z","shell.execute_reply":"2024-02-04T14:09:10.888959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Crema = \"/kaggle/input/cremad/AudioWAV/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nSavee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:10.891055Z","iopub.execute_input":"2024-02-04T14:09:10.891391Z","iopub.status.idle":"2024-02-04T14:09:10.898057Z","shell.execute_reply.started":"2024-02-04T14:09:10.891366Z","shell.execute_reply":"2024-02-04T14:09:10.896937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"# Ravdees","metadata":{}},{"cell_type":"code","source":"file_emotion = []\nfile_path = []\nfor i in ravdess_directory_list:\n    # as their are 24 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(ravdess + i)\n    for f in actor:\n        part = f.split('.')[0].split('-')\n    # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(ravdess + i + '/' + f)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:10.901366Z","iopub.execute_input":"2024-02-04T14:09:10.901762Z","iopub.status.idle":"2024-02-04T14:09:11.116058Z","shell.execute_reply.started":"2024-02-04T14:09:10.901733Z","shell.execute_reply":"2024-02-04T14:09:11.115345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the first element of the 'actor' list\nprint(actor[0])\n\n# Print the first element of the 'part' list\nprint(part[0])\n\n# Print the first element of the 'file_path' list\nprint(file_path[0])\n\n# Convert the third element of the 'part' list to an integer and print it\nprint(int(part[2]))\n\n# Print the value stored in the variable 'f'\nprint(f)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:11.117030Z","iopub.execute_input":"2024-02-04T14:09:11.117327Z","iopub.status.idle":"2024-02-04T14:09:11.122705Z","shell.execute_reply.started":"2024-02-04T14:09:11.117302Z","shell.execute_reply":"2024-02-04T14:09:11.121764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nravdess_df = pd.concat([emotion_df, path_df], axis=1)\n# changing integers to actual emotions.\nravdess_df.Emotions.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust',\n                             8:'surprise'},\n                            inplace=True)\nprint(ravdess_df.head())\nprint(\"______________________________________________\")\nprint(ravdess_df.tail())\nprint(\"_______________________________________________\")\nprint(ravdess_df.Emotions.value_counts())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:11.124138Z","iopub.execute_input":"2024-02-04T14:09:11.124520Z","iopub.status.idle":"2024-02-04T14:09:11.156278Z","shell.execute_reply.started":"2024-02-04T14:09:11.124488Z","shell.execute_reply":"2024-02-04T14:09:11.155419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Crema DataFrame","metadata":{}},{"cell_type":"markdown","source":"CREMA-D is a collection of 7,442 original video clips featuring 91 actors. These actors, ranging from 20 to 74 years old, include 48 males and 43 females from various racial and ethnic backgrounds (African American, Asian, Caucasian, Hispanic, and Unspecified). The actors delivered 12 different sentences expressing six emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) at four emotion levels (Low, Medium, High, and Unspecified).","metadata":{}},{"cell_type":"code","source":"crema_directory_list = os.listdir(Crema)\n\nfile_emotion = []\nfile_path = []\n\nfor file in crema_directory_list:\n    # storing file paths\n    file_path.append(Crema + file)\n    # storing file emotions\n    part=file.split('_')\n    if part[2] == 'SAD':\n        file_emotion.append('sad')\n    elif part[2] == 'ANG':\n        file_emotion.append('angry')\n    elif part[2] == 'DIS':\n        file_emotion.append('disgust')\n    elif part[2] == 'FEA':\n        file_emotion.append('fear')\n    elif part[2] == 'HAP':\n        file_emotion.append('happy')\n    elif part[2] == 'NEU':\n        file_emotion.append('neutral')\n    else:\n        file_emotion.append('Unknown')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nCrema_df = pd.concat([emotion_df, path_df], axis=1)\nCrema_df.head()\nprint(Crema_df.Emotions.value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:11.157508Z","iopub.execute_input":"2024-02-04T14:09:11.157858Z","iopub.status.idle":"2024-02-04T14:09:11.322035Z","shell.execute_reply.started":"2024-02-04T14:09:11.157826Z","shell.execute_reply":"2024-02-04T14:09:11.321156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESS dataset","metadata":{}},{"cell_type":"markdown","source":"Two actresses, aged 26 and 64, spoke 200 target words in the phrase \"Say the word _\" while expressing seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). Recordings were made for each emotion, resulting in a total of 2800 audio files in WAV format. The dataset is organized with separate folders for each actress, containing their emotions, and within those folders, you can find the audio files for all 200 target words.","metadata":{}},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nfile_emotion = []\nfile_path = []\n\nfor dir in tess_directory_list:\n    directories = os.listdir(Tess + dir)\n    for file in directories:\n        part = file.split('.')[0]\n        part = part.split('_')[2]\n        if part=='ps':\n            file_emotion.append('surprise')\n        else:\n            file_emotion.append(part)\n        file_path.append(Tess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nTess_df = pd.concat([emotion_df, path_df], axis=1)\nTess_df.head()\nprint(Tess_df.Emotions.value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:11.323150Z","iopub.execute_input":"2024-02-04T14:09:11.323451Z","iopub.status.idle":"2024-02-04T14:09:11.622284Z","shell.execute_reply.started":"2024-02-04T14:09:11.323426Z","shell.execute_reply":"2024-02-04T14:09:11.621344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SAVEE Dataset","metadata":{}},{"cell_type":"markdown","source":"The SAVEE database was recorded by four English-speaking men (DC, JE, JK, KL) who were postgraduate students and researchers at the University of Surrey, aged 27 to 31. Emotions are categorized as anger, disgust, fear, happiness, sadness, surprise, and neutral. We included neutral to have recordings for all 7 emotion categories. The text material included 15 TIMIT sentences for each emotion, with 3 common, 2 emotion-specific, and 10 generic sentences for each emotion. This resulted in a total of 120 utterances per speaker, covering various emotions and neutral expressions.","metadata":{}},{"cell_type":"code","source":"savee_directory_list = os.listdir(Savee)\n\nfile_emotion = []\nfile_path = []\n\nfor file in savee_directory_list:\n    file_path.append(Savee + file)\n    part = file.split('_')[1]\n    ele = part[:-6]\n    if ele=='a':\n        file_emotion.append('angry')\n    elif ele=='d':\n        file_emotion.append('disgust')\n    elif ele=='f':\n        file_emotion.append('fear')\n    elif ele=='h':\n        file_emotion.append('happy')\n    elif ele=='n':\n        file_emotion.append('neutral')\n    elif ele=='sa':\n        file_emotion.append('sad')\n    else:\n        file_emotion.append('surprise')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nSavee_df = pd.concat([emotion_df, path_df], axis=1)\nSavee_df.head()\nprint(Savee_df.Emotions.value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:11.623507Z","iopub.execute_input":"2024-02-04T14:09:11.624026Z","iopub.status.idle":"2024-02-04T14:09:11.678148Z","shell.execute_reply.started":"2024-02-04T14:09:11.623999Z","shell.execute_reply":"2024-02-04T14:09:11.677280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Integration","metadata":{}},{"cell_type":"code","source":"# creating Dataframe using all the 4 dataframes we created so far.\ndata_path = pd.concat([ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:11.679366Z","iopub.execute_input":"2024-02-04T14:09:11.679730Z","iopub.status.idle":"2024-02-04T14:09:11.748224Z","shell.execute_reply.started":"2024-02-04T14:09:11.679696Z","shell.execute_reply":"2024-02-04T14:09:11.747397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_path.Emotions.value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:11.749205Z","iopub.execute_input":"2024-02-04T14:09:11.749509Z","iopub.status.idle":"2024-02-04T14:09:11.756641Z","shell.execute_reply.started":"2024-02-04T14:09:11.749487Z","shell.execute_reply":"2024-02-04T14:09:11.755750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualisation and Exploration","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Ensure 'Emotions' is treated as a categorical variable\ndata_path['Emotions'] = data_path['Emotions'].astype('category')\n\n# Specify the data parameter in countplot\nplt.title('Count of Emotions', size=16)\nsns.countplot(data=data_path, x='Emotions')\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-02-04T14:09:11.757865Z","iopub.execute_input":"2024-02-04T14:09:11.758121Z","iopub.status.idle":"2024-02-04T14:09:12.007350Z","shell.execute_reply.started":"2024-02-04T14:09:11.758099Z","shell.execute_reply":"2024-02-04T14:09:12.006506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data,sr = librosa.load(file_path[0])\nsr","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:12.012388Z","iopub.execute_input":"2024-02-04T14:09:12.012825Z","iopub.status.idle":"2024-02-04T14:09:20.803825Z","shell.execute_reply.started":"2024-02-04T14:09:12.012800Z","shell.execute_reply":"2024-02-04T14:09:20.802871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ipd.Audio(data,rate=sr)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:20.805015Z","iopub.execute_input":"2024-02-04T14:09:20.805725Z","iopub.status.idle":"2024-02-04T14:09:20.821074Z","shell.execute_reply.started":"2024-02-04T14:09:20.805686Z","shell.execute_reply":"2024-02-04T14:09:20.819964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nlog_spectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(log_spectrogram, y_axis='mel', sr=sr, x_axis='time');\nplt.title('Mel Spectrogram ')\nplt.colorbar(format='%+2.0f dB')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:20.822266Z","iopub.execute_input":"2024-02-04T14:09:20.822639Z","iopub.status.idle":"2024-02-04T14:09:22.609349Z","shell.execute_reply.started":"2024-02-04T14:09:20.822588Z","shell.execute_reply":"2024-02-04T14:09:22.608474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=30)\n\n\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(data,rate=sr)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:22.610688Z","iopub.execute_input":"2024-02-04T14:09:22.611560Z","iopub.status.idle":"2024-02-04T14:09:22.959488Z","shell.execute_reply.started":"2024-02-04T14:09:22.611525Z","shell.execute_reply":"2024-02-04T14:09:22.958489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data augmentation","metadata":{}},{"cell_type":"code","source":"# NOISE\ndef noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\n# STRETCH\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n# SHIFT\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n# PITCH\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:22.960705Z","iopub.execute_input":"2024-02-04T14:09:22.960991Z","iopub.status.idle":"2024-02-04T14:09:22.967933Z","shell.execute_reply.started":"2024-02-04T14:09:22.960966Z","shell.execute_reply":"2024-02-04T14:09:22.966976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NORMAL AUDIO\n\n\nimport librosa.display\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveshow(y=data, sr=sr)\nipd.Audio(data,rate=sr)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:22.969042Z","iopub.execute_input":"2024-02-04T14:09:22.969351Z","iopub.status.idle":"2024-02-04T14:09:23.508535Z","shell.execute_reply.started":"2024-02-04T14:09:22.969327Z","shell.execute_reply":"2024-02-04T14:09:23.507643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUDIO WITH NOISE\nx = noise(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveshow(y=x, sr=sr)\nipd.Audio(x, rate=sr)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:23.509817Z","iopub.execute_input":"2024-02-04T14:09:23.510177Z","iopub.status.idle":"2024-02-04T14:09:24.061332Z","shell.execute_reply.started":"2024-02-04T14:09:23.510145Z","shell.execute_reply":"2024-02-04T14:09:24.060435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade librosa\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:24.062468Z","iopub.execute_input":"2024-02-04T14:09:24.062769Z","iopub.status.idle":"2024-02-04T14:09:37.286178Z","shell.execute_reply.started":"2024-02-04T14:09:24.062744Z","shell.execute_reply":"2024-02-04T14:09:37.284971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip show librosa","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:37.287875Z","iopub.execute_input":"2024-02-04T14:09:37.288190Z","iopub.status.idle":"2024-02-04T14:09:48.925591Z","shell.execute_reply.started":"2024-02-04T14:09:37.288162Z","shell.execute_reply":"2024-02-04T14:09:48.924524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\n\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(y=data, rate=rate)\n\n# Assuming 'data' and 'sr' are already defined\nx = stretch(data)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveshow(y=x, sr=sr)\nipd.Audio(x, rate=sr)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:48.927061Z","iopub.execute_input":"2024-02-04T14:09:48.927382Z","iopub.status.idle":"2024-02-04T14:09:50.210145Z","shell.execute_reply.started":"2024-02-04T14:09:48.927353Z","shell.execute_reply":"2024-02-04T14:09:50.209109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SHIFTED AUDIO\nx = shift(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveshow(y=x, sr=sr)\nipd.Audio(x, rate=sr)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:50.211636Z","iopub.execute_input":"2024-02-04T14:09:50.212338Z","iopub.status.idle":"2024-02-04T14:09:50.779311Z","shell.execute_reply.started":"2024-02-04T14:09:50.212300Z","shell.execute_reply":"2024-02-04T14:09:50.778323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor)\n\n# Assuming 'data' and 'sr' are already defined\nx = pitch(data, sr)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveshow(y=x, sr=sr)\nipd.Audio(x, rate=sr)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:50.780473Z","iopub.execute_input":"2024-02-04T14:09:50.780947Z","iopub.status.idle":"2024-02-04T14:09:51.338741Z","shell.execute_reply.started":"2024-02-04T14:09:50.780910Z","shell.execute_reply":"2024-02-04T14:09:51.337644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature extraction","metadata":{}},{"cell_type":"code","source":"def zcr(data,frame_length,hop_length):\n    zcr=librosa.feature.zero_crossing_rate(data,frame_length=frame_length,hop_length=hop_length)\n    return np.squeeze(zcr)\ndef rmse(data,frame_length=2048,hop_length=512):\n    rmse=librosa.feature.rms(y=data,frame_length=frame_length,hop_length=hop_length)\n    return np.squeeze(rmse)\ndef mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n    mfcc=librosa.feature.mfcc(data,sr=sr)\n    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n\ndef extract_features(data,sr=22050,frame_length=2048,hop_length=512):\n    result=np.array([])\n    \n    result=np.hstack((result,\n                      zcr(data,frame_length,hop_length),\n                      rmse(data,frame_length,hop_length),\n                      mfcc(data,sr,frame_length,hop_length)\n                     ))\n    return result\n\ndef get_features(path,duration=2.5, offset=0.6):\n    data,sr=librosa.load(path,duration=duration,offset=offset)\n    aud=extract_features(data)\n    audio=np.array(aud)\n    \n    noised_audio=noise(data)\n    aud2=extract_features(noised_audio)\n    audio=np.vstack((audio,aud2))\n    \n    pitched_audio=pitch(data,sr)\n    aud3=extract_features(pitched_audio)\n    audio=np.vstack((audio,aud3))\n    \n    pitched_audio1=pitch(data,sr)\n    pitched_noised_audio=noise(pitched_audio1)\n    aud4=extract_features(pitched_noised_audio)\n    audio=np.vstack((audio,aud4))\n    \n    return audio\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:51.340074Z","iopub.execute_input":"2024-02-04T14:09:51.340367Z","iopub.status.idle":"2024-02-04T14:09:51.351800Z","shell.execute_reply.started":"2024-02-04T14:09:51.340341Z","shell.execute_reply":"2024-02-04T14:09:51.350833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing as mp\nprint(\"Number of processors: \", mp.cpu_count())","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:51.353138Z","iopub.execute_input":"2024-02-04T14:09:51.353522Z","iopub.status.idle":"2024-02-04T14:09:51.364779Z","shell.execute_reply.started":"2024-02-04T14:09:51.353488Z","shell.execute_reply":"2024-02-04T14:09:51.363959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Noraml way to get features","metadata":{}},{"cell_type":"code","source":"import timeit\nimport librosa\nimport numpy as np\nfrom tqdm import tqdm\n\n# Assuming your get_features, zcr, rmse, and noise functions are defined somewhere\n\ndef extract_features(data, sr=22050, frame_length=2048, hop_length=512):\n    result = np.array([])\n    result = np.hstack((result,\n                        zcr(data, frame_length, hop_length),\n                        rmse(data, frame_length, hop_length),\n                        my_mfcc(data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n                        ))\n    return result\n\ndef my_mfcc(data, sr, n_fft=2048, hop_length=512, flatten: bool = True):\n    mfcc_result = librosa.feature.mfcc(y=data, sr=sr, n_fft=n_fft, hop_length=hop_length)\n    return np.squeeze(mfcc_result.T) if not flatten else np.ravel(mfcc_result.T)\n\nstart = timeit.default_timer()\n\nX, Y = [], []\n\nfor index, (path, emotion) in tqdm(enumerate(zip(data_path.Path, data_path.Emotions))):\n    features = get_features(path)\n    \n    for i in features:\n        X.append(i)\n        Y.append(emotion)\n\n    if index % 500 == 0:\n        tqdm.write(f'{index} audio has been processed')\n\nprint('Done')\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:09:51.366034Z","iopub.execute_input":"2024-02-04T14:09:51.366290Z","iopub.status.idle":"2024-02-04T14:55:30.511833Z","shell.execute_reply.started":"2024-02-04T14:09:51.366268Z","shell.execute_reply":"2024-02-04T14:55:30.510563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Faster way to get features\n","metadata":{}},{"cell_type":"code","source":"\"\"\"from joblib import Parallel, delayed\nimport timeit\nstart = timeit.default_timer()\n# Define a function to get features for a single audio file\ndef process_feature(path, emotion):\n    features = get_features(path)\n    X = []\n    Y = []\n    for ele in features:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)\n    return X, Y\n\npaths = data_path.Path\nemotions = data_path.Emotions\n\n# Run the loop in parallel\nresults = Parallel(n_jobs=-1)(delayed(process_feature)(path, emotion) for (path, emotion) in zip(paths, emotions))\n\n# Collect the results\nX = []\nY = []\nfor result in results:\n    x, y = result\n    X.extend(x)\n    Y.extend(y)\n\n\nstop = timeit.default_timer()\n\nprint('Time: ', stop - start)    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:55:30.513839Z","iopub.execute_input":"2024-02-04T14:55:30.514579Z","iopub.status.idle":"2024-02-04T14:55:30.525522Z","shell.execute_reply.started":"2024-02-04T14:55:30.514537Z","shell.execute_reply":"2024-02-04T14:55:30.524108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X), len(Y), data_path.Path.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:55:30.527112Z","iopub.execute_input":"2024-02-04T14:55:30.527526Z","iopub.status.idle":"2024-02-04T14:55:30.537401Z","shell.execute_reply.started":"2024-02-04T14:55:30.527485Z","shell.execute_reply":"2024-02-04T14:55:30.535891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving features","metadata":{}},{"cell_type":"code","source":"Emotions = pd.DataFrame(X)\nEmotions['Emotions'] = Y\nEmotions.to_csv('emotion.csv', index=False)\nEmotions.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:55:30.539200Z","iopub.execute_input":"2024-02-04T14:55:30.539832Z","iopub.status.idle":"2024-02-04T14:59:13.239618Z","shell.execute_reply.started":"2024-02-04T14:55:30.539788Z","shell.execute_reply":"2024-02-04T14:59:13.238496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Emotions = pd.read_csv('./emotion.csv')\nEmotions.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:13.240896Z","iopub.execute_input":"2024-02-04T14:59:13.241176Z","iopub.status.idle":"2024-02-04T14:59:40.745885Z","shell.execute_reply.started":"2024-02-04T14:59:13.241152Z","shell.execute_reply":"2024-02-04T14:59:40.744934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Emotions.isna().any())\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:40.747210Z","iopub.execute_input":"2024-02-04T14:59:40.747502Z","iopub.status.idle":"2024-02-04T14:59:40.853938Z","shell.execute_reply.started":"2024-02-04T14:59:40.747476Z","shell.execute_reply":"2024-02-04T14:59:40.853028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Emotions=Emotions.fillna(0)\nprint(Emotions.isna().any())\nEmotions.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:40.854944Z","iopub.execute_input":"2024-02-04T14:59:40.855243Z","iopub.status.idle":"2024-02-04T14:59:41.348634Z","shell.execute_reply.started":"2024-02-04T14:59:40.855217Z","shell.execute_reply":"2024-02-04T14:59:41.347745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(Emotions.isna())","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:41.349913Z","iopub.execute_input":"2024-02-04T14:59:41.350644Z","iopub.status.idle":"2024-02-04T14:59:41.514660Z","shell.execute_reply.started":"2024-02-04T14:59:41.350593Z","shell.execute_reply":"2024-02-04T14:59:41.513670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"#taking all rows and all cols without last col for X which include features\n#taking last col for Y, which include the emotions\n\n\nX = Emotions.iloc[: ,:-1].values\nY = Emotions['Emotions'].values","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:41.515835Z","iopub.execute_input":"2024-02-04T14:59:41.516170Z","iopub.status.idle":"2024-02-04T14:59:41.800761Z","shell.execute_reply.started":"2024-02-04T14:59:41.516141Z","shell.execute_reply":"2024-02-04T14:59:41.799946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As this is a multiclass classification problem onehotencoding our Y\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:41.801969Z","iopub.execute_input":"2024-02-04T14:59:41.802285Z","iopub.status.idle":"2024-02-04T14:59:41.829647Z","shell.execute_reply.started":"2024-02-04T14:59:41.802258Z","shell.execute_reply":"2024-02-04T14:59:41.828942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Y.shape)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:41.830682Z","iopub.execute_input":"2024-02-04T14:59:41.830955Z","iopub.status.idle":"2024-02-04T14:59:41.837312Z","shell.execute_reply.started":"2024-02-04T14:59:41.830926Z","shell.execute_reply":"2024-02-04T14:59:41.836395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42,test_size=0.2, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:41.838420Z","iopub.execute_input":"2024-02-04T14:59:41.838769Z","iopub.status.idle":"2024-02-04T14:59:43.175776Z","shell.execute_reply.started":"2024-02-04T14:59:41.838739Z","shell.execute_reply":"2024-02-04T14:59:43.174775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reshape for lstm\nX_train = x_train.reshape(x_train.shape[0] , x_train.shape[1] , 1)\nX_test = x_test.reshape(x_test.shape[0] , x_test.shape[1] , 1)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:43.177232Z","iopub.execute_input":"2024-02-04T14:59:43.177923Z","iopub.status.idle":"2024-02-04T14:59:43.182937Z","shell.execute_reply.started":"2024-02-04T14:59:43.177885Z","shell.execute_reply":"2024-02-04T14:59:43.181820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaling our data with sklearn's Standard scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:43.192266Z","iopub.execute_input":"2024-02-04T14:59:43.192586Z","iopub.status.idle":"2024-02-04T14:59:44.597983Z","shell.execute_reply.started":"2024-02-04T14:59:43.192560Z","shell.execute_reply":"2024-02-04T14:59:44.596935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM,BatchNormalization , GRU\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import SGD","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:44.599316Z","iopub.execute_input":"2024-02-04T14:59:44.599635Z","iopub.status.idle":"2024-02-04T14:59:44.607470Z","shell.execute_reply.started":"2024-02-04T14:59:44.599592Z","shell.execute_reply":"2024-02-04T14:59:44.606262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\nmodel_checkpoint = ModelCheckpoint('best_model1_weights.h5', monitor='val_accuracy', save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:44.608811Z","iopub.execute_input":"2024-02-04T14:59:44.609116Z","iopub.status.idle":"2024-02-04T14:59:44.617531Z","shell.execute_reply.started":"2024-02-04T14:59:44.609090Z","shell.execute_reply":"2024-02-04T14:59:44.616582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nearly_stop=EarlyStopping(monitor='val_acc',mode='auto',patience=5,restore_best_weights=True)\nlr_reduction=ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.5,min_lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:44.618764Z","iopub.execute_input":"2024-02-04T14:59:44.619080Z","iopub.status.idle":"2024-02-04T14:59:44.631456Z","shell.execute_reply.started":"2024-02-04T14:59:44.619044Z","shell.execute_reply":"2024-02-04T14:59:44.630544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"markdown","source":"Model that have lstm layers take alot of time if you have much free time enjoy with it","metadata":{}},{"cell_type":"code","source":"\"\"\"model01=Sequential()\nmodel01.add(LSTM(128,return_sequences=True,input_shape=(x_train.shape[1],1)))\nmodel01.add(Dropout(0.2))\nmodel01.add(LSTM(128,return_sequences=True))\n#model01.add(Dropout(0.2))\nmodel01.add(LSTM(128,return_sequences=True))\n#model01.add(Dropout(0.2))\nmodel01.add(LSTM(128,return_sequences=True))\n#model01.add(Dropout(0.2))\nmodel01.add(LSTM(128,return_sequences=True))\n#model01.add(Dropout(0.2))\nmodel01.add(LSTM(128,return_sequences=True))\n#model01.add(Dropout(0.3))\nmodel01.add(LSTM(128))\n#model01.add(Dropout(0.3))\nmodel01.add(Dense(7,activation = 'softmax'))\nmodel01.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel01.summary()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:44.632910Z","iopub.execute_input":"2024-02-04T14:59:44.633340Z","iopub.status.idle":"2024-02-04T14:59:44.644437Z","shell.execute_reply.started":"2024-02-04T14:59:44.633306Z","shell.execute_reply":"2024-02-04T14:59:44.643646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"hist=model01.fit(X_train, y_train,\n            epochs=20,\n            validation_data=(X_test, y_test),batch_size=64,\n            verbose=1)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:44.645558Z","iopub.execute_input":"2024-02-04T14:59:44.645915Z","iopub.status.idle":"2024-02-04T14:59:44.654725Z","shell.execute_reply.started":"2024-02-04T14:59:44.645883Z","shell.execute_reply":"2024-02-04T14:59:44.653857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"print(\"Accuracy of our model on test data : \" , model01.evaluate(X_test,y_test)[1]*100 , \"%\")\nepochs = [i for i in range(20)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = hist.history['accuracy']\ntrain_loss = hist.history['loss']\ntest_acc = hist.history['val_accuracy']\ntest_loss = hist.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:44.655791Z","iopub.execute_input":"2024-02-04T14:59:44.656081Z","iopub.status.idle":"2024-02-04T14:59:44.665665Z","shell.execute_reply.started":"2024-02-04T14:59:44.656046Z","shell.execute_reply":"2024-02-04T14:59:44.664865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN model","metadata":{}},{"cell_type":"code","source":"#Reshape for CNN_LSTM MODEL\n\nx_traincnn =np.expand_dims(x_train, axis=2)\nx_testcnn= np.expand_dims(x_test, axis=2)\nx_traincnn.shape, y_train.shape, x_testcnn.shape, y_test.shape\n#x_testcnn[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:44.666540Z","iopub.execute_input":"2024-02-04T14:59:44.666858Z","iopub.status.idle":"2024-02-04T14:59:44.676156Z","shell.execute_reply.started":"2024-02-04T14:59:44.666835Z","shell.execute_reply":"2024-02-04T14:59:44.675342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers as L\n\nmodel = tf.keras.Sequential([\n    L.Conv1D(512,kernel_size=5, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1],1)),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(512,kernel_size=5,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the second max pooling layer\n    \n    L.Conv1D(256,kernel_size=5,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(256,kernel_size=3,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fourth max pooling layer\n    \n    L.Conv1D(128,kernel_size=3,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fifth max pooling layer\n    \n    L.Flatten(),\n    L.Dense(512,activation='relu'),\n    L.BatchNormalization(),\n    L.Dense(7,activation='softmax')\n])\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:44.677472Z","iopub.execute_input":"2024-02-04T14:59:44.677764Z","iopub.status.idle":"2024-02-04T14:59:45.686293Z","shell.execute_reply.started":"2024-02-04T14:59:44.677740Z","shell.execute_reply":"2024-02-04T14:59:45.685244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(x_traincnn, y_train, epochs=50, validation_data=(x_testcnn, y_test), batch_size=64,callbacks=[early_stop,lr_reduction,model_checkpoint])","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:45.687751Z","iopub.execute_input":"2024-02-04T14:59:45.688555Z","iopub.status.idle":"2024-02-04T16:26:40.877253Z","shell.execute_reply.started":"2024-02-04T14:59:45.688517Z","shell.execute_reply":"2024-02-04T16:26:40.876274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:40.878681Z","iopub.execute_input":"2024-02-04T16:26:40.878993Z","iopub.status.idle":"2024-02-04T16:26:49.331584Z","shell.execute_reply.started":"2024-02-04T16:26:40.878965Z","shell.execute_reply":"2024-02-04T16:26:49.330658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\npred_test0 = model.predict(x_testcnn)\ny_pred0 = encoder.inverse_transform(pred_test0)\ny_test0 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred0.flatten()\ndf0['Actual Labels'] = y_test0.flatten()\n\ndf0.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:49.332945Z","iopub.execute_input":"2024-02-04T16:26:49.333289Z","iopub.status.idle":"2024-02-04T16:26:56.486083Z","shell.execute_reply.started":"2024-02-04T16:26:49.333260Z","shell.execute_reply":"2024-02-04T16:26:56.485164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df0","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:56.487167Z","iopub.execute_input":"2024-02-04T16:26:56.487472Z","iopub.status.idle":"2024-02-04T16:26:56.497321Z","shell.execute_reply.started":"2024-02-04T16:26:56.487446Z","shell.execute_reply":"2024-02-04T16:26:56.496403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CLSTM Model","metadata":{}},{"cell_type":"code","source":"#Build the model\n\n# define model\n\"\"\"model000 = Sequential()\nmodel000.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(X.shape[1], 1)))\nmodel000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel000.add(BatchNormalization())\nmodel000.add(Dropout(0.3))\n\n          \nmodel000.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel000.add(BatchNormalization())\nmodel000.add(Dropout(0.3))\n\nmodel000.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel000.add(BatchNormalization())\nmodel000.add(Dropout(0.3))\n          \nmodel000.add(LSTM(128, return_sequences=True)) \nmodel000.add(Dropout(0.3))\n\nmodel000.add(LSTM(128, return_sequences=True)) \nmodel000.add(Dropout(0.3))\nmodel000.add(LSTM(128))\nmodel000.add(Dropout(0.3))\n\nmodel000.add(Dense(128, activation='relu'))\n#model000.add(Dropout(0.3))\n\nmodel000.add(Dense(64, activation='relu'))\n#model000.add(Dropout(0.3))\n\nmodel000.add(Dense(32, activation='relu'))\n#model000.add(Dropout(0.3))\n\nmodel000.add(Dense(7, activation='softmax'))\n\n\n\nmodel000.summary()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:56.498643Z","iopub.execute_input":"2024-02-04T16:26:56.499019Z","iopub.status.idle":"2024-02-04T16:26:56.510908Z","shell.execute_reply.started":"2024-02-04T16:26:56.498988Z","shell.execute_reply":"2024-02-04T16:26:56.509969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"from keras.utils.vis_utils import plot_model\nplot_model( model000, show_shapes=True, show_layer_names=True, to_file='model000.png')\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:56.512146Z","iopub.execute_input":"2024-02-04T16:26:56.512569Z","iopub.status.idle":"2024-02-04T16:26:56.525364Z","shell.execute_reply.started":"2024-02-04T16:26:56.512538Z","shell.execute_reply":"2024-02-04T16:26:56.524485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"model000.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:56.526574Z","iopub.execute_input":"2024-02-04T16:26:56.527249Z","iopub.status.idle":"2024-02-04T16:26:56.536323Z","shell.execute_reply.started":"2024-02-04T16:26:56.527218Z","shell.execute_reply":"2024-02-04T16:26:56.535553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"hist1=model000.fit(x_traincnn, y_train, batch_size=64, epochs=40, validation_data=(x_testcnn, y_test))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:56.537441Z","iopub.execute_input":"2024-02-04T16:26:56.537763Z","iopub.status.idle":"2024-02-04T16:26:56.546806Z","shell.execute_reply.started":"2024-02-04T16:26:56.537739Z","shell.execute_reply":"2024-02-04T16:26:56.545949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"print(\"Accuracy of our model on test data : \" , model000.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\nepochs = [i for i in range(40)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = hist1.history['accuracy']\ntrain_loss = hist1.history['loss']\ntest_acc = hist1.history['val_accuracy']\ntest_loss = hist1.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:56.547882Z","iopub.execute_input":"2024-02-04T16:26:56.548144Z","iopub.status.idle":"2024-02-04T16:26:56.558006Z","shell.execute_reply.started":"2024-02-04T16:26:56.548122Z","shell.execute_reply":"2024-02-04T16:26:56.557188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\n\"\"\"pred_test00 = model000.predict(x_testcnn)\ny_pred00 = encoder.inverse_transform(pred_test)\ny_test00 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred00.flatten()\ndf0['Actual Labels'] = y_test00.flatten()\n\ndf0.head(10)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:56.558862Z","iopub.execute_input":"2024-02-04T16:26:56.559134Z","iopub.status.idle":"2024-02-04T16:26:56.571860Z","shell.execute_reply.started":"2024-02-04T16:26:56.559111Z","shell.execute_reply":"2024-02-04T16:26:56.570969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evalutation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report\ncm = confusion_matrix(y_test0, y_pred0)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n#cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='.2f')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()\nprint(classification_report(y_test0, y_pred0))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:56.573069Z","iopub.execute_input":"2024-02-04T16:26:56.573792Z","iopub.status.idle":"2024-02-04T16:26:57.469247Z","shell.execute_reply.started":"2024-02-04T16:26:56.573768Z","shell.execute_reply":"2024-02-04T16:26:57.468301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving Best Model","metadata":{}},{"cell_type":"code","source":"# MLP for Pima Indians Dataset Serialize to JSON and HDF5\nfrom tensorflow.keras.models import Sequential, model_from_json\nmodel_json = model.to_json()\nwith open(\"CNN_model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"CNN_model_weights.h5\")\nprint(\"Saved model to disk\") ","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:57.470536Z","iopub.execute_input":"2024-02-04T16:26:57.471243Z","iopub.status.idle":"2024-02-04T16:26:57.551437Z","shell.execute_reply.started":"2024-02-04T16:26:57.471194Z","shell.execute_reply":"2024-02-04T16:26:57.550377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, model_from_json\njson_file = open('/kaggle/working/CNN_model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"/kaggle/working/best_model1_weights.h5\")\nprint(\"Loaded model from disk\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:57.552629Z","iopub.execute_input":"2024-02-04T16:26:57.552953Z","iopub.status.idle":"2024-02-04T16:26:57.849623Z","shell.execute_reply.started":"2024-02-04T16:26:57.552928Z","shell.execute_reply":"2024-02-04T16:26:57.848667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nscore = loaded_model.evaluate(x_testcnn,y_test)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:26:57.850887Z","iopub.execute_input":"2024-02-04T16:26:57.851163Z","iopub.status.idle":"2024-02-04T16:27:05.596962Z","shell.execute_reply.started":"2024-02-04T16:26:57.851140Z","shell.execute_reply":"2024-02-04T16:27:05.596038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving and Loading our Stnadrad Scaler and encoder","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Saving scaler\nwith open('scaler2.pickle', 'wb') as f:\n    pickle.dump(scaler, f)\n\n# Loading scaler\nwith open('scaler2.pickle', 'rb') as f:\n    scaler2 = pickle.load(f)\n\n# Saving encoder\nwith open('encoder2.pickle', 'wb') as f:\n    pickle.dump(encoder, f)\n\n# Loading encoder\nwith open('encoder2.pickle', 'rb') as f:\n    encoder2 = pickle.load(f)\n\n    \nprint(\"Done\")    ","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:27:05.598202Z","iopub.execute_input":"2024-02-04T16:27:05.598497Z","iopub.status.idle":"2024-02-04T16:27:05.606593Z","shell.execute_reply.started":"2024-02-04T16:27:05.598471Z","shell.execute_reply":"2024-02-04T16:27:05.605686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test script","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, model_from_json\njson_file = open('/kaggle/working/CNN_model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"/kaggle/working/best_model1_weights.h5\")\nprint(\"Loaded model from disk\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:27:05.607806Z","iopub.execute_input":"2024-02-04T16:27:05.608467Z","iopub.status.idle":"2024-02-04T16:27:06.286237Z","shell.execute_reply.started":"2024-02-04T16:27:05.608434Z","shell.execute_reply":"2024-02-04T16:27:06.285300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/working/scaler2.pickle', 'rb') as f:\n    scaler2 = pickle.load(f)\n    \nwith open('/kaggle/working/encoder2.pickle', 'rb') as f:\n    encoder2 = pickle.load(f)\n\n    \nprint(\"Done\")    ","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:27:06.287689Z","iopub.execute_input":"2024-02-04T16:27:06.288135Z","iopub.status.idle":"2024-02-04T16:27:06.294985Z","shell.execute_reply.started":"2024-02-04T16:27:06.288101Z","shell.execute_reply":"2024-02-04T16:27:06.294079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:27:06.296218Z","iopub.execute_input":"2024-02-04T16:27:06.296569Z","iopub.status.idle":"2024-02-04T16:27:06.304688Z","shell.execute_reply.started":"2024-02-04T16:27:06.296537Z","shell.execute_reply":"2024-02-04T16:27:06.303863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def zcr(data,frame_length,hop_length):\n    zcr=librosa.feature.zero_crossing_rate(data,frame_length=frame_length,hop_length=hop_length)\n    return np.squeeze(zcr)\ndef rmse(data,frame_length=2048,hop_length=512):\n    rmse=librosa.feature.rms(data,frame_length=frame_length,hop_length=hop_length)\n    return np.squeeze(rmse)\ndef mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n    mfcc=librosa.feature.mfcc(data,sr=sr)\n    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n\ndef extract_features(data,sr=22050,frame_length=2048,hop_length=512):\n    result=np.array([])\n    \n    result=np.hstack((result,\n                      zcr(data,frame_length,hop_length),\n                      rmse(data,frame_length,hop_length),\n                      mfcc(data,sr,frame_length,hop_length)\n                     ))\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:27:06.305764Z","iopub.execute_input":"2024-02-04T16:27:06.307931Z","iopub.status.idle":"2024-02-04T16:27:06.316261Z","shell.execute_reply.started":"2024-02-04T16:27:06.307898Z","shell.execute_reply":"2024-02-04T16:27:06.315429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predict_feat(path):\n    d, s_rate= librosa.load(path, duration=2.5, offset=0.6)\n    res=extract_features(d)\n    result=np.array(res)\n    result=np.reshape(result,newshape=(1,2376))\n    i_result = scaler2.transform(result)\n    final_result=np.expand_dims(i_result, axis=2)\n    \n    return final_result","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:30:30.902399Z","iopub.execute_input":"2024-02-04T16:30:30.902784Z","iopub.status.idle":"2024-02-04T16:30:30.908798Z","shell.execute_reply.started":"2024-02-04T16:30:30.902751Z","shell.execute_reply":"2024-02-04T16:30:30.907679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use the function get_predict_feat to extract features from the audio file and print the shape of the resulting feature array.","metadata":{}},{"cell_type":"code","source":"def rmse(data, frame_length=2048, hop_length=512):\n    rmse = librosa.feature.rms(data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(rmse)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:30:33.713340Z","iopub.execute_input":"2024-02-04T16:30:33.714000Z","iopub.status.idle":"2024-02-04T16:30:33.718541Z","shell.execute_reply.started":"2024-02-04T16:30:33.713967Z","shell.execute_reply":"2024-02-04T16:30:33.717646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predicts the emotion label from the input image path using a loaded model and prints the predicted emotion.\n","metadata":{}},{"cell_type":"code","source":"emotions1={1:'Neutral', 2:'Calm', 3:'Happy', 4:'Sad', 5:'Angry', 6:'Fear', 7:'Disgust',8:'Surprise'}\ndef prediction(path1):\n    res=get_predict_feat(path1)\n    predictions=loaded_model.predict(res)\n    y_pred = encoder2.inverse_transform(predictions)\n    print(y_pred[0][0])    ","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:30:36.251922Z","iopub.execute_input":"2024-02-04T16:30:36.252575Z","iopub.status.idle":"2024-02-04T16:30:36.258125Z","shell.execute_reply.started":"2024-02-04T16:30:36.252545Z","shell.execute_reply":"2024-02-04T16:30:36.257091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Perform emotion prediction on the audio file using a pre-trained model\n","metadata":{}},{"cell_type":"code","source":"def mfcc(data, sr, frame_length=2048, hop_length=512, flatten=True):\n    mfcc_result = librosa.feature.mfcc(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n    return np.squeeze(mfcc_result.T) if not flatten else np.ravel(mfcc_result.T)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:34:16.073890Z","iopub.execute_input":"2024-02-04T16:34:16.074801Z","iopub.status.idle":"2024-02-04T16:34:16.080057Z","shell.execute_reply.started":"2024-02-04T16:34:16.074767Z","shell.execute_reply":"2024-02-04T16:34:16.078967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:34:20.167061Z","iopub.execute_input":"2024-02-04T16:34:20.167939Z","iopub.status.idle":"2024-02-04T16:34:20.747813Z","shell.execute_reply.started":"2024-02-04T16:34:20.167905Z","shell.execute_reply":"2024-02-04T16:34:20.746814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-05-01-02-02-01.wav\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:34:24.950687Z","iopub.execute_input":"2024-02-04T16:34:24.951471Z","iopub.status.idle":"2024-02-04T16:34:25.131520Z","shell.execute_reply.started":"2024-02-04T16:34:24.951438Z","shell.execute_reply":"2024-02-04T16:34:25.130271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_21/03-01-04-02-02-02-21.wav\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:34:27.643057Z","iopub.execute_input":"2024-02-04T16:34:27.643432Z","iopub.status.idle":"2024-02-04T16:34:27.817580Z","shell.execute_reply.started":"2024-02-04T16:34:27.643403Z","shell.execute_reply":"2024-02-04T16:34:27.816534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_02/03-01-06-01-02-02-02.wav\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:34:30.018958Z","iopub.execute_input":"2024-02-04T16:34:30.019693Z","iopub.status.idle":"2024-02-04T16:34:30.184811Z","shell.execute_reply.started":"2024-02-04T16:34:30.019662Z","shell.execute_reply":"2024-02-04T16:34:30.183775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-08-01-01-01-01.wav\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:34:32.465403Z","iopub.execute_input":"2024-02-04T16:34:32.466063Z","iopub.status.idle":"2024-02-04T16:34:32.627894Z","shell.execute_reply.started":"2024-02-04T16:34:32.466028Z","shell.execute_reply":"2024-02-04T16:34:32.626655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-07-01-01-01-01.wav\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:34:37.680505Z","iopub.execute_input":"2024-02-04T16:34:37.680876Z","iopub.status.idle":"2024-02-04T16:34:37.844953Z","shell.execute_reply.started":"2024-02-04T16:34:37.680846Z","shell.execute_reply":"2024-02-04T16:34:37.843677Z"},"trusted":true},"execution_count":null,"outputs":[]}]}